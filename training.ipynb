{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I43bMheNrinl",
        "outputId": "ea9ead82-07b9-4300-f41b-d341b96f0ce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\python311\\lib\\site-packages (4.28.1)\n",
            "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\python311\\lib\\site-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python311\\lib\\site-packages (from transformers) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\python311\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\python311\\lib\\site-packages (from transformers) (2023.5.5)\n",
            "Requirement already satisfied: requests in c:\\python311\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\python311\\lib\\site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in c:\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python311\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mK68mWhRRJ2K"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForPreTraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m7XqNGZ-RMPA"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JBHj4Vt1RPAJ"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hbT1ZS8aRacN"
      },
      "outputs": [],
      "source": [
        "class LIABertClassifier(nn.Module):\n",
        "    def __init__(self,model,num_labels):\n",
        "        super(LIABertClassifier,self).__init__()\n",
        "        self.bert = model.bert\n",
        "        self.config = model.config\n",
        "        self.num_labels = num_labels\n",
        "        self.cls = nn.Linear(self.config.hidden_size,num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        ) ->Tuple[torch.Tensor]:\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0][:,0,:]\n",
        "        prediction = self.cls(sequence_output)\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bq6S_zrpRb8r"
      },
      "outputs": [],
      "source": [
        "model_base= AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = LIABertClassifier(model=model_base,num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wlvIlOKKtCPm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "slnILsqwGxTX"
      },
      "outputs": [],
      "source": [
        "cols = [\"Datetime\",\"Text\",\"Likes\",\"Retweets\",\"Feeling\"]\n",
        "data = pd.read_csv(\n",
        "    r\"C:\\Users\\allan\\Downloads\\drive-download-20230505T001753Z-001\\final.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\",\n",
        "    index_col = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "REdK4z4YG9kZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "data.drop([\"Datetime\",\"Likes\",\"Retweets\"],\n",
        "          axis=1,\n",
        "          inplace=True)\n",
        "data = data.drop(0)\n",
        "data = data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ekphrasis in c:\\users\\allan\\appdata\\roaming\\python\\python311\\site-packages (0.5.4)\n",
            "Requirement already satisfied: termcolor in c:\\python311\\lib\\site-packages (from ekphrasis) (2.3.0)\n",
            "Requirement already satisfied: tqdm in c:\\python311\\lib\\site-packages (from ekphrasis) (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from ekphrasis) (0.4.6)\n",
            "Requirement already satisfied: ujson in c:\\python311\\lib\\site-packages (from ekphrasis) (5.7.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\allan\\appdata\\roaming\\python\\python311\\site-packages (from ekphrasis) (3.7.1)\n",
            "Requirement already satisfied: nltk in c:\\users\\allan\\appdata\\roaming\\python\\python311\\site-packages (from ekphrasis) (3.8.1)\n",
            "Requirement already satisfied: ftfy in c:\\python311\\lib\\site-packages (from ekphrasis) (6.1.1)\n",
            "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (from ekphrasis) (1.24.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\allan\\appdata\\roaming\\python\\python311\\site-packages (from ftfy->ekphrasis) (0.2.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\allan\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->ekphrasis) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\allan\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\python311\\lib\\site-packages (from matplotlib->ekphrasis) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from matplotlib->ekphrasis) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\python311\\lib\\site-packages (from matplotlib->ekphrasis) (9.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python311\\lib\\site-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\python311\\lib\\site-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: click in c:\\users\\allan\\appdata\\roaming\\python\\python311\\site-packages (from nltk->ekphrasis) (8.1.3)\n",
            "Requirement already satisfied: joblib in c:\\python311\\lib\\site-packages (from nltk->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python311\\lib\\site-packages (from nltk->ekphrasis) (2023.5.5)\n",
            "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ekphrasis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.dicts.emoticons import emoticons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\allan\\AppData\\Roaming\\Python\\Python311\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading twitter - 1grams ...\n"
          ]
        }
      ],
      "source": [
        "text_processor = TextPreProcessor(\n",
        "    \n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    \n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    \n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    # tokenizer=tokenizer.tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "pattern = r\"<.*?>\" #pattern used by ekphrasis to mark social network lingo\n",
        "\n",
        "#function that only tokenizes what is not from the ekphrasis pattern\n",
        "#this nedded to be done so we could use the portuguese tokenizer and keep\n",
        "#all the ekphrasis tags\n",
        "def ekphrasis_tokenize(text):\n",
        "    \n",
        "    ptext = text_processor.pre_process_doc(text)\n",
        "    \n",
        "    pattern_matches = re.findall(pattern, ptext)\n",
        "    \n",
        "    tokens = []\n",
        "    prev_end = 0\n",
        "    for match in pattern_matches:\n",
        "        start, end = re.search(re.escape(match), ptext).span()\n",
        "        tokens.extend(tokenizer.tokenize(ptext[prev_end:start]))\n",
        "        tokens.append(match)\n",
        "        prev_end = end\n",
        "    tokens.extend(tokenizer.tokenize(ptext[prev_end:]))\n",
        "    \n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BlbZpy0HHiV",
        "outputId": "218309a0-e117-47d2-a86c-6090cc41f19e"
      },
      "outputs": [],
      "source": [
        "data_clean = data.copy()\n",
        "data_clean.Text = [ekphrasis_tokenize(tweet) for tweet in data.Text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "H6SOj46BHKEk"
      },
      "outputs": [],
      "source": [
        "data_labels = data_clean.Feeling.values\n",
        "data_labels[data_labels == 'Pos'] = 1\n",
        "data_labels[data_labels == 'Neu'] = 0.5\n",
        "data_labels[data_labels == 'Neg'] = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nfIXbrL4uppG"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R5Z5ts1gni4V"
      },
      "outputs": [],
      "source": [
        "shuffle=np.random.randint(0,len(data_clean['Text']),1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "M95GVXYMUfZ-"
      },
      "outputs": [],
      "source": [
        "ytrain_global = np.array(data_clean['Feeling'].tolist())[shuffle]\n",
        "xtrain_global = np.array(data_clean['Text'])[shuffle]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q78hzrsEvexz",
        "outputId": "b2b65f32-2795-4322-d94f-0e5522feb5dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('SÃ©rio que alguÃ©m teve a audÃ¡cia de criticar a Alessandra Negrini no carnaval? Eu li isso mesmo?? Uma das mais deusas das deusas! Essa galera sÃ³ pode estar de sacanagem! Mas, sociedade jovencÃªntrica ne? A mulher depois dos 40 sÃ³ pode mesmo ousar na limpeza da casa. Meu c*!!!!!',\n",
              " 0.0)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xtrain_global[1],ytrain_global[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0QcffYzjOBO"
      },
      "outputs": [],
      "source": [
        "import sklearn.model_selection as model_selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGthp7btjrD6"
      },
      "outputs": [],
      "source": [
        "xtrain, xval, ytrain, yval = model_selection.train_test_split(xtrain_global, ytrain_global, test_size=0.30, random_state=42,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4KpAFs9jxIu"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(xtrain.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "val_encodings = tokenizer(xval.tolist(), truncation=True, padding=True,max_length=512, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0v6SonpkD73"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        label = torch.tensor(self.labels[idx].astype('float32'))\n",
        "        return (item,label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sBluXDxkTzF"
      },
      "outputs": [],
      "source": [
        "ds_train = MyDataset(train_encodings,ytrain)\n",
        "ds_val   = MyDataset(val_encodings,yval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myvv1f8KkXhy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ9Wya45kbn1"
      },
      "outputs": [],
      "source": [
        "dl_train = DataLoader(ds_train,shuffle=True,batch_size=8)\n",
        "dl_eval  = DataLoader(ds_val,batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leLZtyGCkn_z"
      },
      "outputs": [],
      "source": [
        "x,y = next(iter(dl_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGQo2IYrkcHH"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cmGNuDlkeA2"
      },
      "outputs": [],
      "source": [
        "batch = {k: v.to(device) for k, v in x.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npVuzTlQkfJ0",
        "outputId": "d1315c30-90a2-484a-facc-e75a499183b8"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrkMyjHMkqsY"
      },
      "outputs": [],
      "source": [
        "out = model(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uejqQuIfktj_"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=5e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaspM2o0lDcU"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "num_training_steps = num_epochs * len(dl_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLsiC2emlCjS"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTgVnMM7lD4n"
      },
      "outputs": [],
      "source": [
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ulFEQmZlGMd"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_1yr5KxlR1f"
      },
      "outputs": [],
      "source": [
        "loss_fct = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsKQTKQ9lNH7"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for epoch in range(num_epochs):\n",
        "    count+=1\n",
        "    lepochs = []\n",
        "    for batch,y in dl_train:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        y     = y.to(device)\n",
        "        outputs = model(**batch)\n",
        "        loss = loss_fct(outputs,y.to(torch.long))\n",
        "        lepochs.append(loss.cpu().item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(np.mean(lepochs))\n",
        "    torch.save(model.state_dict(),f'./model{count}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyWQ0fW3lvx0"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv3FsDAZlOCO"
      },
      "outputs": [],
      "source": [
        "ytrue = []\n",
        "ypred = []\n",
        "for batch,y in dl_eval:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "    predictions = torch.argmax(outputs, dim=-1)\n",
        "    ytrue += y.tolist()\n",
        "    ypred += predictions.cpu().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB9FdzPgyc6d"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),'/content/drive/MyDrive/model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfZSVbJjzeuV"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(backup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od0wcXeflyXD"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UKHK_wUlzi6"
      },
      "outputs": [],
      "source": [
        "metrics.confusion_matrix(ytrue,ypred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4cSy9ill0n3"
      },
      "outputs": [],
      "source": [
        "print(metrics.classification_report(ytrue,ypred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
